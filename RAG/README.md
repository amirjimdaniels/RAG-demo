# Minimal RAG Demo

A simple demonstration of Retrieval Augmented Generation (RAG) comparing retrieval-based answers vs naive keyword matching. Includes a **web UI** for side-by-side comparison.

## ğŸš€ Quick Start

### Web UI (Recommended)
```bash
pip install -r requirements.txt
cd rag_demo
python app.py
```
Then open http://localhost:5000 in your browser.

### Command Line
```bash
python rag_demo/main.py          # Uses dummy LLM (no API key needed)
python rag_demo/main.py deepseek # Uses DeepSeek API
python rag_demo/main.py groq     # Uses Groq API
```

## ğŸ“Š What This Demo Shows

The demo compares:
- **Without RAG**: Naive keyword matching (often gets wrong answers)
- **With RAG**: Semantic retrieval using embeddings (finds the right context)

Currently uses a **DummyLLM** that returns retrieved context as-is - no actual LLM API calls are made by default. This is intentional to show that the *retrieval* is the key differentiator.

## ğŸ†“ Free LLM Options

| Provider | Free Tier | API Key | Notes |
|----------|-----------|---------|-------|
| **DeepSeek** | âœ… Yes | [Get Key](https://platform.deepseek.com/) | Generous free tier, great for testing |
| **Groq** | âœ… Yes | [Get Key](https://console.groq.com/) | Very fast inference, Llama 3.3 70B free |
| **Together AI** | âœ… $5 credit | [Get Key](https://api.together.xyz/) | Many open-source models |
| **Ollama** | âœ… 100% Free | None (local) | Run models locally, requires [Ollama](https://ollama.ai/) |
| **MiniMax** | âœ… Free tier | [Get Key](https://api.minimax.chat/) | MiniMax-M2.1 model |
| **Google Gemini** | âœ… Free tier | [Get Key](https://makersuite.google.com/app/apikey) | 60 requests/min free |

### Using Free LLMs

```bash
# DeepSeek (recommended - truly free)
export DEEPSEEK_API_KEY=your_key
python rag_demo/main.py deepseek

# Groq (fast & free)
export GROQ_API_KEY=your_key
python rag_demo/main.py groq

# MiniMax (free tier)
export MINIMAX_API_KEY=your_key
python rag_demo/main.py minimax

# Together AI ($5 free credit)
export TOGETHER_API_KEY=your_key
python rag_demo/main.py together

# Ollama (local, completely free)
# First: ollama pull llama3.2
python rag_demo/main.py ollama
```

## ğŸ’° Paid LLM Options

```bash
# OpenAI (GPT-3.5/4)
export OPENAI_API_KEY=your_key
python rag_demo/main.py openai

# Anthropic (Claude)
export ANTHROPIC_API_KEY=your_key
python rag_demo/main.py anthropic

# Google (Gemini Pro)
export GOOGLE_API_KEY=your_key
python rag_demo/main.py google
```

## ğŸ“ Project Structure

```
rag_demo/
â”œâ”€â”€ __init__.py          # Package marker
â”œâ”€â”€ app.py               # Flask web UI for comparison
â”œâ”€â”€ main.py              # CLI demo script
â”œâ”€â”€ data.py              # Sample dataset, questions & ground truth
â”œâ”€â”€ embeddings.py        # TF-IDF vectorization utilities
â”œâ”€â”€ evaluation.py        # Shared prompt building, LLM-as-judge, scoring
â”œâ”€â”€ llms.py              # LLM provider classes (base class + all providers)
â”œâ”€â”€ retriever_numpy.py   # Vector search with NumPy
â”œâ”€â”€ retriever_minimal.py # Lightweight pure-Python cosine similarity
â””â”€â”€ templates/
    â””â”€â”€ index.html       # Web UI template
```

## ğŸ”§ How It Works

This demo implements several modern RAG techniques:

1. **Embedding**: Questions and documents are converted to vector representations. By default, a SentenceTransformer model (e.g., `BAAI/bge-small-en-v1.5`) is used for semantic search, but it can fall back to TF-IDF if unavailable.
2. **Chunking**: Documents are split into overlapping chunks or sentences to improve retrieval granularity.
3. **Retrieval**: Cosine similarity is used to find the most relevant chunks for a given question. NumPy-accelerated and pure-Python retrievers are both available.
4. **Reranking (Cross-Encoder)**: Optionally, a cross-encoder model (e.g., `cross-encoder/ms-marco-MiniLM-L-6-v2`) reranks the top retrieved chunks for higher accuracy.
5. **Two-Pass Retrieval (Query Expansion)**: For real LLMs, the system can use a two-pass approach: it first retrieves context and gets an initial answer, then asks the LLM to generate an improved search query, and retrieves again with this expanded query.
6. **Map-Reduce Retrieval**: For large documents, the system retrieves all relevant chunks above a similarity threshold, summarizes each chunk with the LLM (map), and then combines or further summarizes these summaries (reduce) to produce a focused context.
7. **Generation**: The LLM (or DummyLLM) uses the retrieved context to answer the question.

The key insight: **good retrieval = good answers**, even with a simple LLM. Advanced strategies like reranking and multi-step retrieval further boost answer quality, especially on long or complex documents.

## Requirements

- Python 3.8+
- flask
- numpy
- python-dotenv
- requests
- scikit-learn

## ğŸ” API Key Storage (Local Only)

The web UI stores API keys **in a client-side cookie** (per provider) so you don't have to re-enter them.
When you run the app locally (http://localhost:5000), the server also **writes the key to a local .env** file
the first time you use it in the UI. This keeps keys on your machine and avoids committing secrets.

**Important:**
- Do not commit .env to source control.
- Cookies are stored locally in your browser (SameSite=Lax).

## ğŸ”„ Restarting the Server

To restart the web server after making changes or if you encounter issues:

1. **Stop the server**: Press `Ctrl+C` in the terminal where the server is running.
2. **(Optional) Clear cache**: Delete any `__pycache__` folders if you want a clean state.
3. **Start the server**:
   ```bash
   cd rag_demo
   python app.py
   ```
   Then open [http://localhost:5000](http://localhost:5000) in your browser.

If you see errors on startup:
- Ensure all dependencies are installed: `pip install -r requirements.txt`
- If you deleted or modified files in `rag_demo/templates/`, restore them or re-copy from backup.
- Check that your Python environment is activated (if using a virtualenv).
